{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import keras\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dropout,LeakyReLU,concatenate,ZeroPadding2D,BatchNormalization,Conv2DTranspose, GlobalAveragePooling2D\n",
    "from keras.layers import Add\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import os \n",
    "import cv2 \n",
    "from matplotlib.pyplot import imshow,title ,show\n",
    "from keras.applications import Xception\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy \n",
    "from keras.callbacks import ModelCheckpoint,Callback\n",
    "from keras.optimizers import Adam, SGD\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Dropout, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers \n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import LeakyReLU,Add,ZeroPadding2D\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 768\n",
    "model = UXception(input_shape=(img_size,img_size,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def dice_score(true,pred):\n",
    "    return 1-(distance.dice(true.ravel(),pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_crossentropy(y_true, y_pred):\n",
    "\n",
    "    y_pred_f = K.reshape(y_pred, (batch_size*img_deps*img_rows,3))\n",
    "    y_true_f = K.reshape(y_true, (batch_size*img_deps*img_rows,2))\n",
    "\n",
    "    soft_pred_f = K.softmax(y_pred_f)\n",
    "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
    "\n",
    "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
    "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
    "\n",
    "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
    "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
    "\n",
    "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
    "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
    "\n",
    "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return (binary_crossentropy(y_true, y_pred)+dice_loss(y_true, y_pred))\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attach_attention_module(net, attention_module):\n",
    "    if attention_module == 'se_block': # SE_block\n",
    "        net = se_block(net)\n",
    "    elif attention_module == 'cbam_block': # CBAM_block\n",
    "        net = cbam_block(net)\n",
    "    else:\n",
    "        raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n",
    "\n",
    "    return net\n",
    "\n",
    "def se_block(input_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n",
    "    As described in https://arxiv.org/abs/1709.01507.\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "\n",
    "    se_feature = GlobalAveragePooling2D()(input_feature)\n",
    "    se_feature = Reshape((1, 1, channel))(se_feature)\n",
    "    assert se_feature._keras_shape[1:] == (1,1,channel)\n",
    "    se_feature = Dense(channel // ratio,\n",
    "                       activation='relu',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       use_bias=True,\n",
    "                       bias_initializer='zeros')(se_feature)\n",
    "    assert se_feature._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    se_feature = Dense(channel,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       use_bias=True,\n",
    "                       bias_initializer='zeros')(se_feature)\n",
    "    assert se_feature._keras_shape[1:] == (1,1,channel)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se_feature = Permute((3, 1, 2))(se_feature)\n",
    "\n",
    "    se_feature = multiply([input_feature, se_feature])\n",
    "    return se_feature\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "    \n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature._keras_shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature._keras_shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool._keras_shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool._keras_shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat._keras_shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat) \n",
    "    assert cbam_feature._keras_shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = attach_attention_module(x,attention_module='cbam_block')\n",
    "    if activation == True:\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16):\n",
    "    x = LeakyReLU(alpha=0.1)(blockInput)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = attach_attention_module(x,attention_module='cbam_block')\n",
    "    blockInput = BatchNormalization()(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    #x = attach_attention_module(x,attention_module='cbam_block')\n",
    "    return x\n",
    "\n",
    "def UXception(input_shape=(None, None, 3),dropout_rate=0.5):\n",
    "\n",
    "    backbone = Xception(input_shape=input_shape,weights='imagenet',include_top=False)\n",
    "    input = backbone.input\n",
    "    start_neurons = 16\n",
    "\n",
    "    conv4 = backbone.layers[121].output\n",
    "    conv4 = LeakyReLU(alpha=0.1)(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout_rate)(pool4)\n",
    "    \n",
    "     # Middle\n",
    "    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = LeakyReLU(alpha=0.1)(convm)\n",
    "    \n",
    "    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(dropout_rate)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n",
    "    \n",
    "    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    conv3 = backbone.layers[31].output\n",
    "    uconv3 = concatenate([deconv3, conv3])    \n",
    "    uconv3 = Dropout(dropout_rate)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    conv2 = backbone.layers[21].output\n",
    "    conv2 = ZeroPadding2D(((1,0),(1,0)))(conv2)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "        \n",
    "    uconv2 = Dropout(0.1)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    conv1 = backbone.layers[11].output\n",
    "    conv1 = ZeroPadding2D(((3,0),(3,0)))(conv1)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    \n",
    "    uconv1 = Dropout(0.1)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n",
    "    \n",
    "    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n",
    "    uconv0 = Dropout(dropout_rate)(uconv0)\n",
    "    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n",
    "    \n",
    "    uconv0 = Dropout(dropout_rate/2)(uconv0)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
    "    \n",
    "    model = Model(input, output_layer)\n",
    "    model.name = 'u-xception'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.load('./Numpy/float_x_unet_train.npy', mmap_mode='r')\n",
    "y_train=np.load('./Numpy/float_y_unet_train.npy', mmap_mode='r')\n",
    "\n",
    "x_val=np.load('./Numpy/float_x_unet_val.npy', mmap_mode='r')\n",
    "y_val=np.load('./Numpy/float_y_unet_val.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x_train.shape, y_train.shape)\n",
    "print (x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train_2=np.zeros(x_train.shape, dtype='float16')\n",
    "for idx, i in enumerate(x_train):\n",
    "    x_train_2[idx]=i.astype('float16')/255\n",
    "    print (idx)\n",
    "print (x_train_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_val_2=np.zeros(x_val.shape, dtype='float16')\n",
    "for idx, i in enumerate(x_val):\n",
    "    x_val_2[idx]=i.astype('float16')/255\n",
    "    print (idx)\n",
    "print (x_val_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train=y_train.astype('float16')/255\n",
    "y_val=y_val.astype('float16')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train=np.expand_dims(y_train,axis=-1)\n",
    "y_val=np.expand_dims(y_val,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train[y_train>0.]=1\n",
    "y_val[y_val>0.]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 768\n",
    "model = UXception(input_shape=(img_size,img_size,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler,ReduceLROnPlateau,CSVLogger,ModelCheckpoint\n",
    "\n",
    "adam=Adam(lr=1e-4)\n",
    "model.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy',dice_coef])\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join('./Weights/', 'UNET_attention_tb.hdf5'),\n",
    "        monitor='val_loss', mode='min',save_best_only=True, verbose=1)\n",
    "\n",
    "#Callback for reducing learning rate based on validation loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=4, verbose=1, min_lr=1e-6)\n",
    "\n",
    "#Callback for storing logs of learning rate, loss and accuracy\n",
    "csvlogger=CSVLogger('./CSV/logs_from_scratch_adam_tb.csv')\n",
    "\n",
    "callbacks = [model_checkpoint,reduce_lr,csvlogger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=150,verbose=1,callbacks=callbacks,batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./Weights/segmentation-Copy1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op2=model.predict(x_val_2,verbose=1,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(op2)):\n",
    "#     print(np.unique(y_val[i]))\n",
    "#     print(np.unique(np.argmax(op2[i],axis=-1)))\n",
    "    print (i)\n",
    "    plt.figure(dpi=250)\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow((y_val[i][:,:,0]*255).astype('uint8'))\n",
    "#     plt.imshow((x_val[i][:,:,0]*255).astype('uint8'))\n",
    "#     plt.subplot(1,4,2)\n",
    "#     plt.imshow((y_val[i][:,:,0]*255).astype('uint8'))\n",
    "#     plt.subplot(1,4,3)\n",
    "#     plt.imshow(x_val[i][:,:,0])\n",
    "    # plt.imshow(op2[i][:,:,1])\n",
    "    plt.subplot(1,4,2)\n",
    "    new_mask=op2[i][:,:,0]>0.5\n",
    "    new_mask=(new_mask*255).astype('uint8')\n",
    "    plt.imshow(new_mask)\n",
    "#     plt.subplot(1,4,4)\n",
    "#     plt.imshow(np.argmax(op2[i],axis=-1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=25\n",
    "tp=0\n",
    "stack=[]\n",
    "\n",
    "for i in range(len(op2)):\n",
    "    if np.max(y_val[i])>0.5:\n",
    "        stack.append(dice_score(y_val[i],np.where(op2[i][:,:,0]>0.5,1,0)))\n",
    "        if np.max(op2[i,:,:,0])>0.5:\n",
    "            tp=tp+1\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for idx, i in enumerate(op2):\n",
    "    if np.max(i)>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "    print (idx)\n",
    "        \n",
    "test=[]\n",
    "for i in y_val:\n",
    "    if np.max(i)>0:\n",
    "        test.append(1)\n",
    "    else:\n",
    "        test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(test, y_pred, target_names=['Healthy', 'Edema']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('./CSV/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.load('./Numpy/x_test.npy')\n",
    "y_test=np.load('./Numpy/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2=np.zeros(x_test.shape, dtype='float16')\n",
    "for idx, i in enumerate(x_test):\n",
    "    x_test_2[idx]=i.astype('float16')/255\n",
    "    print (idx)\n",
    "print (x_test_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.astype('float16')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.expand_dims(y_test,axis=-1)\n",
    "y_test[y_test>0.]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op2=model.predict(x_test_2,verbose=1,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=0\n",
    "stack=[]\n",
    "\n",
    "for i in range(len(op2)):\n",
    "    if np.max(y_test[i])>0.5:\n",
    "        stack.append(dice_score(y_test[i],np.where(op2[i][:,:,0]>0.5,1,0)))\n",
    "        if np.max(op2[i,:,:,0])>0.5:\n",
    "            tp=tp+1\n",
    "    print (i)\n",
    "    \n",
    "print (\"TP:\", tp)\n",
    "print (\"Total Positives:\", len(op2))\n",
    "print (\"Recall:\", float(tp)/len(op2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=list(df_test['filepaths'])\n",
    "labels=list(df_test['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "test=labels[200:]\n",
    "for idx, i in enumerate(filenames[200:]):\n",
    "    print (idx)\n",
    "    image=cv2.imread(i)\n",
    "    image=cv2.resize(image, (768,768))\n",
    "    image=image.astype('float16')/255\n",
    "    pred=model.predict(np.expand_dims(image, axis=0), verbose=1)\n",
    "    \n",
    "    if np.max(pred[0])>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
